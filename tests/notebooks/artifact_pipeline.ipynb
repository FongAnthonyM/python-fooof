{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "\"\"\"artifact_pipeline.py\n",
    "A pipeline for removing artifact.\n",
    "\"\"\"\n",
    "# Package Header #\n",
    "from src.spikedetection.header import *\n",
    "\n",
    "# Header #\n",
    "__author__ = __author__\n",
    "__credits__ = __credits__\n",
    "__maintainer__ = __maintainer__\n",
    "__email__ = __email__\n",
    "\n",
    "# Imports #\n",
    "# Standard Libraries #\n",
    "import importlib\n",
    "import itertools\n",
    "import pathlib\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Third-Party Packages #\n",
    "from dspobjects.plot import Figure, TimeSeriesPlot, SpectraPlot, BarPlot\n",
    "from fooof.sim.gen import gen_aperiodic\n",
    "from fooof import FOOOF, FOOOFGroup\n",
    "import hdf5objects\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import savgol_filter, welch\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import toml\n",
    "import torch\n",
    "from torch import nn\n",
    "from xltektools.hdf5framestructure import XLTEKStudyFrame\n",
    "\n",
    "# Local Packages #\n",
    "from src.spikedetection.artifactrejection.fooof.goodnessauditor import GoodnessAuditor, RSquaredBoundsAudit, SVMAudit\n",
    "from src.spikedetection.artifactrejection.fooof.ooffitter import OOFFitter, iterdim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n",
    "# Definitions #\n",
    "# Data Classes\n",
    "class ElectrodeLead(NamedTuple):\n",
    "    name: str\n",
    "    type: str\n",
    "    contacts: dict\n",
    "\n",
    "\n",
    "# Classes #\n",
    "\n",
    "\n",
    "# Functions #\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def closest_square(n):\n",
    "    n = int(n)\n",
    "    i = int(np.ceil(np.sqrt(n)))\n",
    "    while True:\n",
    "        if (n % i) == 0:\n",
    "            break\n",
    "        i += 1\n",
    "    assert n == (i * (n // i))\n",
    "    return i, n // i\n",
    "\n",
    "\n",
    "def get_lead_groups(el_label, el_type):\n",
    "    assert len(el_label) == len(el_type)\n",
    "\n",
    "    LEAD_NAME_NOID = np.array([''.join(map(lambda c: '' if c in '0123456789' else c, ll))\n",
    "        for ll in el_label])\n",
    "    CONTACT_IX = np.arange(len(el_label))\n",
    "    LEAD_NAME = np.unique(LEAD_NAME_NOID)\n",
    "\n",
    "    lead_group = {}\n",
    "    for l_name in LEAD_NAME:\n",
    "        lead_group[l_name] = \\\n",
    "            {'Contacts': el_label[np.flatnonzero(LEAD_NAME_NOID == l_name)],\n",
    "             'IDs': CONTACT_IX[np.flatnonzero(LEAD_NAME_NOID == l_name)],\n",
    "             'Type': np.unique(el_type[np.flatnonzero(LEAD_NAME_NOID == l_name)])}\n",
    "        assert len(lead_group[l_name]['Type']) == 1\n",
    "\n",
    "        lead_group[l_name]['Type'] = lead_group[l_name]['Type'][0]\n",
    "\n",
    "    return lead_group\n",
    "\n",
    "\n",
    "def make_bipolar(lead_group):\n",
    "    for l_name in lead_group:\n",
    "        sel_lead = lead_group[l_name]\n",
    "        n_contact = len(sel_lead['IDs'])\n",
    "        if 'grid' in sel_lead['Type']:\n",
    "            n_row, n_col = closest_square(n_contact)\n",
    "        else:\n",
    "            n_row, n_col = [n_contact, 1]\n",
    "\n",
    "        CA = np.arange(len(sel_lead['Contacts'])).reshape((n_row, n_col), order='F')\n",
    "\n",
    "        lead_group[l_name]['Contact_Pairs_ix'] = []\n",
    "\n",
    "        if n_row > 1:\n",
    "            for bp1, bp2 in zip(CA[:-1, :].flatten(), CA[1:, :].flatten()):\n",
    "                lead_group[l_name]['Contact_Pairs_ix'].append(\n",
    "                        (sel_lead['IDs'][bp1],\n",
    "                         sel_lead['IDs'][bp2]))\n",
    "\n",
    "        if n_col > 1:\n",
    "            for bp1, bp2 in zip(CA[:, :-1].flatten(), CA[:, 1:].flatten()):\n",
    "                lead_group[l_name]['Contact_Pairs_ix'].append(\n",
    "                        (sel_lead['IDs'][bp1],\n",
    "                         sel_lead['IDs'][bp2]))\n",
    "\n",
    "        \"\"\"\n",
    "        if (n_row > 1) & (n_col > 1):\n",
    "            for bp1, bp2 in zip(CA[:-1, :-1].flatten(), CA[1:, 1:].flatten()):\n",
    "                lead_group[l_name]['Contact_Pairs_ix'].append(\n",
    "                        (sel_lead['IDs'][bp1],\n",
    "                         sel_lead['IDs'][bp2]))\n",
    "        lead_group[l_name]['Contact_Pairs_ix'] = np.array(\n",
    "            lead_group[l_name]['Contact_Pairs_ix'])\n",
    "\n",
    "        lead_group[l_name]['Contact_Pairs_ix'] = \\\n",
    "            lead_group[l_name]['Contact_Pairs_ix'][\n",
    "                np.argsort(lead_group[l_name]['Contact_Pairs_ix'][:, 0])]\n",
    "        \"\"\"\n",
    "\n",
    "    return lead_group\n",
    "\n",
    "\n",
    "def make_bipolar_elecs_all(eleclabels, eleccoords):\n",
    "\n",
    "    lead_group = get_lead_groups(eleclabels[:, 1], eleclabels[:, 2])\n",
    "    lead_group = make_bipolar(lead_group)\n",
    "\n",
    "    bp_elecs_all = {\n",
    "            'IDX': [],\n",
    "            'Anode': [],\n",
    "            'Cathode': [],\n",
    "            'Lead': [],\n",
    "            'Contact': [],\n",
    "            'Contact_Abbr': [],\n",
    "            'Type': [],\n",
    "            'x': [],\n",
    "            'y': [],\n",
    "            'z': []}\n",
    "\n",
    "    for l_name in lead_group:\n",
    "        for el_ix, el_iy in lead_group[l_name]['Contact_Pairs_ix']:\n",
    "            bp_elecs_all['IDX'].append((el_ix, el_iy))\n",
    "            bp_elecs_all['Anode'].append(el_ix)\n",
    "            bp_elecs_all['Cathode'].append(el_iy)\n",
    "\n",
    "            bp_elecs_all['Lead'].append(l_name)\n",
    "            bp_elecs_all['Contact'].append('{}-{}'.format(eleclabels[el_ix, 1], eleclabels[el_iy, 1]))\n",
    "            bp_elecs_all['Contact_Abbr'].append('{}-{}'.format(eleclabels[el_ix, 0], eleclabels[el_iy, 0]))\n",
    "            bp_elecs_all['Type'].append(lead_group[l_name]['Type'])\n",
    "\n",
    "            try:\n",
    "                coord = (eleccoords[el_ix] + eleccoords[el_iy]) / 2\n",
    "            except:\n",
    "                coord = [np.nan, np.nan, np.nan]\n",
    "            bp_elecs_all['x'].append(coord[0])\n",
    "            bp_elecs_all['y'].append(coord[1])\n",
    "            bp_elecs_all['z'].append(coord[2])\n",
    "\n",
    "    bp_elecs_all = pd.DataFrame(bp_elecs_all)\n",
    "    if np.core.numeric.dtype is None:\n",
    "        importlib.reload(np.core.numeric)\n",
    "    return bp_elecs_all.sort_values(by=['Anode', 'Cathode']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_ECoG_sample(study_frame, time_start, time_end):\n",
    "    natus_data = {}\n",
    "\n",
    "    # Get the Sample Rate\n",
    "    if study_frame.validate_sample_rate():\n",
    "        natus_data['fs'] = 1024  #\n",
    "    else:\n",
    "        natus_data['fs'] = 1024\n",
    "\n",
    "    # Get the minimum number of channels present in all recordings\n",
    "    natus_data['min_valid_chan'] = min([shape[1] for shape in study_frame.get_shapes()])\n",
    "\n",
    "    natus_data['data'] = study_frame.find_data_range(time_start, time_end, approx=True)\n",
    "\n",
    "    return natus_data\n",
    "\n",
    "\n",
    "def convert_ECoG_BP(natus_data, BP_ELECS):\n",
    "    natus_data['data'] = (natus_data['data'].data[:, BP_ELECS['Anode'].values] -\n",
    "                          natus_data['data'].data[:, BP_ELECS['Cathode'].values])\n",
    "\n",
    "    return natus_data\n",
    "\n",
    "\n",
    "def half_life(duration, fs_state):\n",
    "    samples = duration / fs_state\n",
    "    return np.exp(-(1/samples)*np.log(2))\n",
    "\n",
    "\n",
    "def do_fitting(foo, freqs, spectrum, freq_range):\n",
    "    foo.add_data(freqs, spectrum, freq_range)\n",
    "    aperiodic_params_ = foo._robust_ap_fit(freqs, spectrum)\n",
    "    ap_fit = gen_aperiodic(freqs, aperiodic_params_)\n",
    "    r_val = np.corrcoef(spectrum, ap_fit)\n",
    "    return r_val[0][1] ** 2\n",
    "\n",
    "\n",
    "def do_fittings(foo, freqs, spectra, freq_range):\n",
    "    r_sq = []\n",
    "    for spectrum in spectra:\n",
    "        r_sq.append(do_fitting(foo, freqs, spectrum, freq_range))\n",
    "\n",
    "    return r_sq\n",
    "\n",
    "\n",
    "def load_data(files, info):\n",
    "    artifact_info = toml.load(info.as_posix())[\"raters\"]\n",
    "    artifact_data = {}\n",
    "\n",
    "    for file in files:\n",
    "        name_parts = file.name.split('.')\n",
    "        subject_id = name_parts[0]\n",
    "        file_number = int(name_parts[2])\n",
    "        artifact_file = loadmat(file.as_posix(), squeeze_me=True)\n",
    "\n",
    "        clip_data = {\n",
    "            \"sample_rate\": artifact_file[\"fs\"],\n",
    "            \"channel_labels\": artifact_file[\"channels\"],\n",
    "            \"time_axis\": artifact_file[\"timestamp vector\"],\n",
    "            \"data\": artifact_file[\"data\"],\n",
    "        }\n",
    "\n",
    "        if subject_id not in artifact_data:\n",
    "            artifact_data[subject_id] = [None] * 10\n",
    "\n",
    "        artifact_data[subject_id][file_number] = clip_data\n",
    "\n",
    "    return artifact_data, artifact_info\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Parameters #\n",
    "SVM_PATH = pathlib.Path.cwd().joinpath(\"all_metric_svm.obj\")\n",
    "ARTIFACT_DIR = pathlib.Path(\"/home/anthonyfong/ProjectData/EpilepsySpikeDetection/Artifact_Review/\")\n",
    "ARTIFACT_INFO = ARTIFACT_DIR.joinpath(\"Artifact_Info.toml\")\n",
    "ARTIFACT_FILES = ARTIFACT_DIR.glob(\"*.mat\")\n",
    "OUT_DIR = pathlib.Path(\"/home/anthonyfong/ProjectData/EpilepsySpikeDetection/Artifact_Review/Images\")\n",
    "TIME_AXIS = 0\n",
    "CHANNEL_AXIS = 1\n",
    "LOWER_FREQUENCY = 1\n",
    "UPPER_FREQUENCY = 250\n",
    "METRICS = {\"r_squared\", \"normal_entropy\", \"mae\", \"mse\", \"rmse\", \"curve_offset\", \"curve_exp\"}\n",
    "BEST_METRICS = {\"r_squared\", \"normal_entropy\", \"mae\", \"rmse\"}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# FOOOF\n",
    "fg = FOOOFGroup(peak_width_limits=[4, 8], min_peak_height=0.05, max_n_peaks=1, verbose=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by the scale function.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 95>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     92\u001B[0m     ag_metrics[name] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(metric_)\n\u001B[1;32m     94\u001B[0m importlib\u001B[38;5;241m.\u001B[39mreload(np\u001B[38;5;241m.\u001B[39mcore\u001B[38;5;241m.\u001B[39mnumeric)  \u001B[38;5;66;03m# Pandas causes numpy to break which is dumb....\u001B[39;00m\n\u001B[0;32m---> 95\u001B[0m metrics_all_scaled \u001B[38;5;241m=\u001B[39m \u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetrics_dataframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m metrics_all_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(metrics_all_scaled, columns\u001B[38;5;241m=\u001B[39mmetrics_dataframe\u001B[38;5;241m.\u001B[39mcolumns)\n",
      "File \u001B[0;32m~/PycharmProjects/venvs/development3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:194\u001B[0m, in \u001B[0;36mscale\u001B[0;34m(X, axis, with_mean, with_std, copy)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscale\u001B[39m(X, \u001B[38;5;241m*\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, with_mean\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, with_std\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;124;03m\"\"\"Standardize a dataset along any axis.\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \n\u001B[1;32m    120\u001B[0m \u001B[38;5;124;03m    Center to the mean and component wise scale to unit variance.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    192\u001B[0m \n\u001B[1;32m    193\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthe scale function\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m with_mean:\n",
      "File \u001B[0;32m~/PycharmProjects/venvs/development3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:805\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    803\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[1;32m    804\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[0;32m--> 805\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    806\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[1;32m    809\u001B[0m         )\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    812\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by the scale function."
     ]
    }
   ],
   "source": [
    "# Aggregate Data #\n",
    "# Load Data\n",
    "artifact_data, artifact_info = load_data(ARTIFACT_FILES, ARTIFACT_INFO)\n",
    "\n",
    "# Create Data Structures\n",
    "ag_reviews = {reviewer[\"name\"]: [] for reviewer in artifact_info}\n",
    "ag_reviews |= {\"Reviewer Intersection\": [], \"Reviewer Union\": []}\n",
    "ag_metrics = {m: [] for m in METRICS}\n",
    "\n",
    "artifact_metrics = {}\n",
    "aggregate_data = {\"reviews\": ag_reviews, \"metrics\": ag_metrics}\n",
    "\n",
    "# Format Data From Files and Create Metrics\n",
    "for subject_id, data in artifact_data.items():\n",
    "    artifact_metrics[subject_id] = [None] * len(data)\n",
    "    for i, artifact_clip in enumerate(data):\n",
    "        # Format Reviewer Data\n",
    "        review_channels = {}\n",
    "        for reviewer in artifact_info:\n",
    "            zero_index = tuple(np.array(reviewer[\"review_channels\"][subject_id][i]) - 1)\n",
    "            review_channels[reviewer[\"name\"]] = zero_index\n",
    "        review_union = set()\n",
    "        review_intersect = set(np.array(artifact_info[0][\"review_channels\"][subject_id][i]) - 1)\n",
    "        for rv in review_channels.values():\n",
    "            review_union |= (set(rv))\n",
    "            review_intersect.intersection_update(set(rv))\n",
    "        review_union = tuple(review_union)\n",
    "        review_intersect = tuple(review_intersect)\n",
    "        reviews = review_channels.copy()\n",
    "        reviews.update({\"Reviewer Intersection\": review_intersect, \"Reviewer Union\": review_union})\n",
    "\n",
    "        # Create Metrics\n",
    "        sample_rate = artifact_clip[\"sample_rate\"]\n",
    "\n",
    "        freqs, spectra = welch(artifact_clip[\"data\"], fs=sample_rate, nperseg=2048, axis=TIME_AXIS)\n",
    "\n",
    "        # Limit Frequency Range\n",
    "        lower_limit = int(np.searchsorted(freqs, LOWER_FREQUENCY, side=\"right\") - 1)\n",
    "        upper_limit = int(np.searchsorted(freqs, UPPER_FREQUENCY, side=\"right\"))\n",
    "\n",
    "\n",
    "        spectra = spectra[(slice(None),) * TIME_AXIS + (slice(lower_limit, upper_limit),)]\n",
    "        freqs = freqs[lower_limit:upper_limit]\n",
    "\n",
    "        # Fitting\n",
    "        fg.fit(freqs, spectra.T)\n",
    "\n",
    "        fg.get_fooof()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        curve_removed = fit_curves.spectra - fit_curves.curves\n",
    "        curve_2 = curve_removed ** 2\n",
    "\n",
    "        prob = curve_2 / np.sum(curve_2, axis=0)\n",
    "        entro = entropy(prob)\n",
    "        normal_entropy = entro / np.log(prob.shape[0])\n",
    "\n",
    "        artifact_metric = {\n",
    "            \"raw_data\": artifact_clip[\"data\"],\n",
    "            \"fit_curves\": fit_curves,\n",
    "            \"r_squared\": fit_curves.r_squared,\n",
    "            \"mae\": fit_curves.mae,\n",
    "            \"mse\": fit_curves.mse,\n",
    "            \"rmse\": fit_curves.rmse,\n",
    "            \"normal_entropy\": normal_entropy,\n",
    "            \"curve_offset\": fit_curves.parameters[0, :],\n",
    "            \"curve_exp\": fit_curves.parameters[1, :],\n",
    "            \"reviews\": reviews,\n",
    "        }\n",
    "\n",
    "        # Load Data into Data Structures\n",
    "        artifact_metrics[subject_id][i] = artifact_metric\n",
    "\n",
    "        ag_metrics[\"r_squared\"] += list(fit_curves.r_squared)\n",
    "        ag_metrics[\"normal_entropy\"] += list(normal_entropy)\n",
    "        ag_metrics[\"mae\"] += list(fit_curves.mae)\n",
    "        ag_metrics[\"mse\"] += list(fit_curves.mse)\n",
    "        ag_metrics[\"rmse\"] += list(fit_curves.rmse)\n",
    "        ag_metrics[\"curve_offset\"] += list(fit_curves.parameters[0, :])\n",
    "        ag_metrics[\"curve_exp\"] += list(fit_curves.parameters[1, :])\n",
    "        for reviewer, channels in reviews.items():\n",
    "            good_channels = np.zeros((fit_curves.spectra.shape[CHANNEL_AXIS],))\n",
    "            good_channels[channels,] = 1\n",
    "            aggregate_data[\"reviews\"][reviewer] += list(good_channels)\n",
    "\n",
    "# Load Data into Pandas Data Frame\n",
    "review_dataframe = pd.DataFrame.from_dict(ag_reviews)\n",
    "metrics_dataframe = pd.DataFrame.from_dict(ag_metrics)\n",
    "for name, metric_ in ag_metrics.items():\n",
    "    ag_metrics[name] = np.array(metric_)\n",
    "\n",
    "importlib.reload(np.core.numeric)  # Pandas causes numpy to break which is dumb....\n",
    "metrics_all_scaled = scale(metrics_dataframe.to_numpy())\n",
    "metrics_all_scaled = pd.DataFrame(metrics_all_scaled, columns=metrics_dataframe.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}